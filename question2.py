# -*- coding: utf-8 -*-
"""M23CSA504-Speech-PA1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EHCMcBhW-BecztZxoyuM-ycAFqgw4quY
"""

import pandas as pd
import os
import csv
import librosa
import librosa.display
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
scaler = StandardScaler()
le = LabelEncoder()

# Unzip dataset
!wget https://zenodo.org/record/1203745/files/UrbanSound8K.tar.gz -O urban8k.tgz
!tar -xzf urban8k.tgz
!rm urban8k.tgz

df = pd.read_csv('/content/UrbanSound8K/metadata/UrbanSound8K.csv')
df.head()

df.groupby('class').slice_file_name.count()

import torch
# Fuction for Hann Window
def hann_window(win_size):
    n = torch.arange(0, win_size, dtype=torch.float32)
    val = 0.5 * (1 - torch.cos(2 * torch.pi * n / (win_size - 1))).numpy()
    return val

# Fuction for Hamming Window
def hamming_window(win_size):
    n = torch.arange(0, win_size, dtype=torch.float32)
    val = (0.54 - 0.46 * torch.cos(2 * torch.pi * n / (win_size - 1))).numpy()
    return val

# Fuction for Rectangular Window
def rectangular_window(win_size):
    val = torch.ones(win_size).numpy()
    return val

def create_dataset_df(csv_file):
    dataset_df = pd.read_csv(csv_file)
    filepaths = []
    for i, row in dataset_df.iterrows():
        filepaths.append(os.path.join('UrbanSound8K/audio', 'fold'+str(row['fold']), row['slice_file_name']))
    dataset_df['filepath'] = filepaths
    return dataset_df

dataset_df = create_dataset_df('/content/UrbanSound8K/metadata/UrbanSound8K.csv')

!pip install mutagen
import mutagen
import mutagen.wave
def get_audio_metadata_mutagen(filepath):
    metadata = {}
    f = mutagen.wave.WAVE(filepath)
    metadata['length'] = f.info.length
    metadata['bitrate'] = f.info.bitrate
    metadata['channels'] = f.info.channels
    metadata['sample_rate'] = f.info.sample_rate
    metadata['bits_per_sample'] = f.info.bits_per_sample
    return metadata


def compute_audio_statistics(dataset_df):
    metadata_dict = {'length': [], 'bitrate': [], 'channels': [], 'sample_rate': [], 'bits_per_sample': []}
    # Extract metadata
    for filepath in dataset_df['filepath']:
        metadata = get_audio_metadata_mutagen(filepath)
        for key in metadata_dict.keys():
            metadata_dict[key].append(metadata[key])
    # Add new columns to dataframe
    for key in metadata_dict.keys():
        dataset_df[key] = metadata_dict[key]

    return dataset_df

dataset_df = dataset_df.drop(columns=['fold', 'slice_file_name', 'fsID', 'start', 'end'])
audio_statistics_df = compute_audio_statistics(dataset_df)
audio_statistics_df.describe()

random_samples = dataset_df.groupby('class').sample(1)
audio_samples, labels = random_samples['filepath'].tolist(), random_samples['class'].tolist()

# Visualize the waveforms
fig, axs = plt.subplots(5, 2, figsize=(15,15))
index = 0
for col in range(2):
    for row in range(5):
        audio_file, sample_rate = librosa.load(audio_samples[index])
        # Use librosa.display.waveshow instead of librosa.display.waveplot
        librosa.display.waveshow(audio_file, sr=sample_rate, ax=axs[row][col])
        axs[row][col].set_title('{}'.format(labels[index]))
        index += 1
fig.tight_layout()

def STFT(win_type):
  n_fft = 1024  # Change n_fft to 1024 to match the window size
  hop_length = 512
  fig, axs = plt.subplots(5, 2, figsize=(20,20))
  index = 0
  n_s = 4
  for col in range(2):
      for row in range(5):
          audio_file, sample_rate = librosa.load(audio_samples[index])

          # Adjust window size to match n_fft or vice-versa
          stft = librosa.stft(y=audio_file, n_fft=n_fft, hop_length=hop_length, window=win_type(n_fft))
          # In this case we're changing n_fft to 1024 and passing that in hann_window

          S_db = librosa.amplitude_to_db(np.abs(stft), ref=np.max)
          librosa.display.specshow(S_db,
                              sr=n_fft,
                              hop_length=hop_length,
                              x_axis="time",
                              y_axis='log',
                              ax=axs[row][col])
          axs[row][col].set_title('{}'.format(labels[index]))
          index += 1
  fig.tight_layout()

STFT(hann_window)

STFT(hamming_window)

STFT(rectangular_window)

# Visualize 40 MFCCs
n_fft = 2048
# Librosa default is n_fft // 4
hop_length = 512
fig, axs = plt.subplots(5, 2, figsize=(20,20))
index = 0
n_s = 4
for col in range(2):
    for row in range(5):
        audio_file, sample_rate = librosa.load(audio_samples[index])
        # The mfcc function in librosa.feature expects keyword arguments, not positional arguments
        mfccs = librosa.feature.mfcc(y=audio_file,
                                    sr=sample_rate,
                                    n_fft=n_fft,
                                    n_mfcc=40)
        librosa.display.specshow(mfccs,
                             sr=n_fft,
                             hop_length=hop_length,
                             x_axis="time",
                             ax=axs[row][col])
        axs[row][col].set_title('{}'.format(labels[index]))
        index += 1
fig.tight_layout()

# Load the audio files and labels
audio_files = []
labels = []

# Load the labels from the csv file
label_dict = {}
with open("/content/UrbanSound8K/metadata/UrbanSound8K.csv") as f:
    reader = csv.reader(f)
    next(reader)  # skip header row
    for row in reader:
        filename = row[0]
        label = row[7]  # label is in the 8th column (index 7)
        label_dict[filename] = label

# Loop through all the folders in the directory
for foldername in os.listdir("/content/UrbanSound8K/audio/"):
    if foldername.endswith(".DS_Store"):  # skip .DS_Store files
        continue
    # Loop through all the audio files in the folder
    for filename in os.listdir("/content/UrbanSound8K/audio/{}".format(foldername)):
        # Skip non-audio files, specifically '.DS_Store'
        if filename == ".DS_Store":
            continue
        # Load the audio file
        data, sr = librosa.load("/content/UrbanSound8K/audio/{}/{}".format(foldername, filename), sr=None)

        # Get the label for the audio file
        label = label_dict[filename]

        audio_files.append((data, sr))
        labels.append(label)

def extract_mfcc(win_func, win_size=1024, hop_size=512):
  X = []
  max_length = 0

  for data, sr in audio_files:
      # Extract MFCCs
      D = librosa.stft(y=data, n_fft=win_size, hop_length=hop_size, window=win_func(win_size))
      mfccs = librosa.feature.mfcc(S=librosa.power_to_db(np.abs(D)), sr=sr, n_mfcc=13)
      mfccs_scaled = np.mean(mfccs.T,axis=0)
      X.append(mfccs_scaled)
  return X

X_hann = extract_mfcc(hann_window)
X_hamming = extract_mfcc(hamming_window)
X_rect = extract_mfcc(rectangular_window)

X_hann = scaler.fit_transform(X_hann)
X_hamming = scaler.fit_transform(X_hamming)
X_rect = scaler.fit_transform(X_rect)

y_hann = le.fit_transform(labels)
y_hamming = le.fit_transform(labels)
y_rect = le.fit_transform(labels)

# Split the data into training and test sets
from sklearn.model_selection import train_test_split
# Train and Test Data for model traning with Hann Window
X_train, X_test, y_train, y_test = train_test_split(X_hann, y_hann, test_size=0.2, random_state=42)

# Train and Test Data for model traning with Hamming Window
X_train, X_test, y_train, y_test = train_test_split(X_hamming, y_hamming, test_size=0.2, random_state=42)

# Train and Test Data for model traning with Reactangular Window
X_train, X_test, y_train, y_test = train_test_split(X_rect, y_rect, test_size=0.2, random_state=42)

# import Classifiers
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier
from sklearn.linear_model import SGDClassifier
from sklearn.neural_network import MLPClassifier
from xgboost import XGBClassifier

from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, ConfusionMatrixDisplay

# Define the classifiers to use
classifiers = [
    KNeighborsClassifier(),
    DecisionTreeClassifier(),
    RandomForestClassifier(),
    AdaBoostClassifier(),
    GradientBoostingClassifier(),
    SGDClassifier(),
    MLPClassifier(),
    XGBClassifier()
]

# Create an empty DataFrame to store the results
results = pd.DataFrame(columns=['Classifier', 'Accuracy', 'Precision', 'Recall'])

# Loop through the classifiers
for clf in classifiers:
    # Fit the classifier to the training data
    clf.fit(X_train, y_train)

    # Evaluate the classifier on the test data
    y_pred = clf.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, average="macro")
    recall = recall_score(y_test, y_pred, average="macro")

    # Store the results in the DataFrame
    results = pd.concat([results, pd.DataFrame([{'Classifier': clf.__class__.__name__,
                              'Accuracy': accuracy,
                              'Precision': precision,
                              'Recall': recall}])], ignore_index=True)


# Sort the DataFrame by the Accuracy column in descending order
results.sort_values(by='Accuracy', ascending=False, inplace=True)

# show the DataFrame
results

# Create and fit the model
model = RandomForestClassifier()
model.fit(X_train, y_train)

# Evaluate the classifier on the test data using the fitted model
y_pred = model.predict(X_test)
cm = confusion_matrix(y_test, y_pred)

# Plot the confusion matrix
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)
disp.plot()
plt.show()

# Function to compute STFT and display spectrogram
def plot_spectrogram(y, sr, window_func, window_size=1024, hop_size=512):
    # Apply STFT with the specified window function
    D = librosa.stft(y, n_fft=window_size, hop_length=hop_size, window=window_func(window_size))
    plt.figure(figsize=(12, 8))
    librosa.display.specshow(librosa.amplitude_to_db(np.abs(D), ref=np.max), y_axis='log', x_axis='time', sr=sr)
    plt.colorbar(format='%+2.0f dB')
    plt.title(f'Spectrogram with {window_func.__name__.capitalize()} Window')
    plt.show()

audio_file = '/content/songs/midwest-emo-song-with-heavy-electronic-vibes-273748.mp3'
y, sr = librosa.load(audio_file, sr=None)
# Generate spectrograms using different windowing techniques
plot_spectrogram(y, sr, hann_window)
plot_spectrogram(y, sr, hamming_window)
plot_spectrogram(y, sr, rectangular_window)

audio_file = '/content/songs/new-wave-pop-genre-283726.mp3'
y, sr = librosa.load(audio_file, sr=None)
# Generate spectrograms using different windowing techniques
plot_spectrogram(y, sr, hann_window)
plot_spectrogram(y, sr, hamming_window)
plot_spectrogram(y, sr, rectangular_window)

audio_file = '/content/songs/panek-thrash-classical-piano-meets-thrash-metal-231488.mp3'
y, sr = librosa.load(audio_file, sr=None)
# Generate spectrograms using different windowing techniques
plot_spectrogram(y, sr, hann_window)
plot_spectrogram(y, sr, hamming_window)
plot_spectrogram(y, sr, rectangular_window)

audio_file = '/content/songs/powerful-epic-rock-248903.mp3'
y, sr = librosa.load(audio_file, sr=None)
# Generate spectrograms using different windowing techniques
plot_spectrogram(y, sr, hann_window)
plot_spectrogram(y, sr, hamming_window)
plot_spectrogram(y, sr, rectangular_window)